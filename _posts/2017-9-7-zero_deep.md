---
layout: post
title: ゼロから作るdeep learning
category: deep_learning
---

## 第1章
+ Anaconda データの分析に重点をおいたディストリビューション
  + NumPy　数値計算のライブラリ
  + Matplotlib グラフ、画像描写のライブラリ
+ データ構造
  + リスト　- スライシング、負の添え字
  + ディクショナリ
+ インデントは4空白推奨
+ numpy.array
  + element wise, ブロードキャスト
  + テンソル
  + 配列によるアクセス

## 第2章
+ パーセプトロン
  + ニューロン/エッジ、入力/パラメータ（重み、閾値）
  + バイアス　→ -閾値
  + パラメータを変更することで様々な機能を実現
  + 単層のパーセプトロン・・・線形領域(AND,OR,NAND)のみ
  + 多層のパーセプトロン・・・非線形領域(XOR)も
+ 多層のパーセプトロン　→　XOR →　コンピュータ
+ パーセプトロンはニューラルネットワークの基礎

## 第3章
+ ニューラルネットワーク
  + パーセプトロンの活性化関数をスッテプ関数から滑らかなものに
  + 活性化関数・・・入力信号→出力信号の変換、活性化（＝発火）のしやすさを決める
  + 単純パーセプトロン・・・単層・ステップ関数
  + 多層パーセプトロン・・・多層・滑らかな活性化関数（シグモイド関数）
  + ステップ関数/シグモイド関数・・・違い：滑らかさ、2値かどうか、共通：入出力の大小関係、0:1の範囲に納める
  + 重要な共通点として非線形関数だということ、線形関数だと隠れ層のないものを作れてしまう
  + 近年では活性化関数として主にReLU関数を使う
  + 伝搬の計算は内積で効率よく計算できる
  + 出力層の活性化関数：回帰問題→恒等関数、2クラス分類問題→シグモイド関数、多クラス分類問題→ソフトマックス関数
  + ソフトマックス関数→出力の各ニューロンは全ての入力信号から受ける、0:1、総和1、大小不
  + クラス分類では出力の最も大きいニューロンに相当するクラスだけを認識結果とするため、出力層の↑よく省略
  + 出力層のニューロンの数は分類したいクラス数にする
  + 推論処理→順方向伝搬
  + pickle→オブジェクトをファイルとして保存
  + 正規化→データをある決まった範囲に納める
  + 数値計算を行うライブラリの多くは大きな配列の計算に高度な最適化をかける、バス帯域の負荷軽減→バッチ処理で高速化

参考文献：斎藤 康毅（1984）『ゼロから作るDeep Learning -Pythonで学ぶディープラーニングの理論と実装』　オライリー・ジャパン．
