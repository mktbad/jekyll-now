---
layout: post
title: ゼロから作るDeepLearning
category: dl!
---

## 第1章
+ Anaconda データの分析に重点をおいたディストリビューション
  + NumPy　数値計算のライブラリ
  + Matplotlib グラフ、画像描写のライブラリ
+ データ構造
  + リスト　- スライシング、負の添え字
  + ディクショナリ
+ インデントは4空白推奨
+ numpy.array
  + element wise, ブロードキャスト
  + テンソル
  + 配列によるアクセス

## 第2章
+ パーセプトロン
  + ニューロン/エッジ、入力/パラメータ（重み、閾値）
  + バイアス　→ -閾値
  + パラメータを変更することで様々な機能を実現
  + 単層のパーセプトロン・・・線形領域(AND,OR,NAND)のみ
  + 多層のパーセプトロン・・・非線形領域(XOR)も
+ 多層のパーセプトロン　→　XOR →　コンピュータ
+ パーセプトロンはニューラルネットワークの基礎

## 第3章
+ ニューラルネットワーク
  + パーセプトロンの活性化関数をスッテプ関数から滑らかなものに
  + 活性化関数・・・入力信号→出力信号の変換、活性化（＝発火）のしやすさを決める
  + 単純パーセプトロン・・・単層・ステップ関数
  + 多層パーセプトロン・・・多層・滑らかな活性化関数（シグモイド関数）
  + ステップ関数/シグモイド関数・・・違い：滑らかさ、2値かどうか、共通：入出力の大小関係、0:1の範囲に納める
  + 重要な共通点として非線形関数だということ、線形関数だと隠れ層のないものを作れてしまう
  + 近年では活性化関数として主にReLU関数を使う
  + 伝搬の計算は内積で効率よく計算できる
  + 出力層の活性化関数：回帰問題→恒等関数、2クラス分類問題→シグモイド関数、多クラス分類問題→ソフトマックス関数
  + ソフトマックス関数→出力の各ニューロンは全ての入力信号から受ける、0:1、総和1、大小不
  + クラス分類では出力の最も大きいニューロンに相当するクラスだけを認識結果とするため、出力層の↑よく省略
  + 出力層のニューロンの数は分類したいクラス数にする
  + 推論処理（フォワード処理）→順方向伝搬
  + pickle→オブジェクトをファイルとして保存
  + 正規化→データをある決まった範囲に納める
  + 数値計算を行うライブラリの多くは大きな配列の計算に高度な最適化をかける、バス帯域の負荷軽減→バッチ処理で高速化

## 第4章
+ NNは損失関数の値が最も小さくなる重みパラメータを勾配を手掛かりに探す
+ パーセプトロンでも線形分離可能な問題であればデータから自動で学習できる
+ 分類のアルゴリズム
  + 人が考えたアルゴリズム
  + 特徴量を設計し、それを用いてデータから得たベクトルを識別器で学習
  + NN・・・特徴量までも機械が学習(end to end machine learning)
+ 過学習していない、汎化能力を正しく評価するために訓練データとテストデータ
+ 全体の訓練データのおおよその近似としてミニバッチ学習
+ 認識精度はほとんどの場所で微分0となりパラメータの更新が止まる、しかも変化は飛び飛び
+ 損失関数はパラメータを少し変化させると連続的に変化
+ ステップ関数を用いるとパラメータの微小な変化がステップ関数によって消えて損失関数の値が変化しなくなる
+ シグモイドはどの場所でも傾きが0にならず良い
+ 数値微分→微小な値を与えた時の差分によって微分を求める
+ 勾配を元に一定の距離だけ移動することを繰り返す→勾配法
  + 更新の幅を決めるパラメータ：学習率
  + 人の手によって設定されるパラメータ→ハイパーパラメータ
  + 重みWと勾配は同じ形状
+ 簡単な関数はlambda記法が使える
+ NNの学習→重みとバイアスを訓練データを元に調整（ミニバッチ→勾配算出→パラメータの更新、繰り返す）
+ 確率的勾配降下法→SGD
+ 分類問題では入力層のニューロンは入力データの数、出力層は分類数の数
+ 誤差逆伝播法で勾配を数値微分よりも高速に求められる
+ エポック→訓練データを全て使い切った回数（max(train_size/batch_size,1)）



参考文献：斎藤 康毅（1984）『ゼロから作るDeep Learning -Pythonで学ぶディープラーニングの理論と実装』　オライリー・ジャパン．
